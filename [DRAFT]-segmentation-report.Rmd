---
title: "DSE5004 Segmentation and Profiling"
author: "Tracey Zicherman"
date: "2024-06-19"
output:
  word_document: default
  html_document:
    df_print: paged
always_allow_html: true
---

```{r setup, include=FALSE}
# set code chunk knit defaults
knitr::opts_chunk$set(include=FALSE)
```

```{r libraries}
library(rpart)
library(rpart.plot)
library(vip)
library(tidyverse)
library(plotly)
library(skimr)
library(DescTools)
```

```{r load_data}
# load datasets
data <- read_csv("Clean_Customer_Dataset.csv")

# basic cleaning
data <- data %>% 
  select(-1) %>% # drop index number column
  mutate(across(where(is.character), as.factor)) %>% # cast characters to factor
  mutate(across(where(is.numeric), ~ ifelse(is.infinite(.), 0, .))) # get rid of subtle and unexplained error where 0's of AvgCarValue are read in as inf

# inspect dataset features and types
data
```

## Executive Summary

***SIDEKICK: TO DO - Do This Last***

## Methodology

In this project we performed a segmentation and profiling analysis using the following step-wise process:

1. ***Data Cleaning***: The data was cleaned and prepared for manipulation.
2. ***Feature Engineering***: New derivative features were created from preexisting features, including a binary feature identifying high retention customers, and some preexisting features were transformed.
3. ***Exploratory Data Analysis***: Extensive investigation of all features was conducted both univariate analysis (including visualizations of all single feature distributions) and bivariate analysis (including some pairwise distribution visualizations and regression modeling).
4. ***Feature Selection***: A custom subset of customer features was chosen for segmentation. The selection criteria employed were chosen to facilitate analysis, segmentation methods and profiling.
5. ***Supervised Segmentation***: Candidate customer segments were generated using a supervised method, with the binary feature `HighRetention` used as the target. The decision tree algorithm was employed to segment the feature space into 8 segments, corresponding to the leaves in the decision tree. The best fitting "pruned" tree was selected, for an optimal balance between relative error and complexity. The decision rules used by the tree algorithm were used on the data to create the corresponding segments.
6. ***Unsupervised Segmentation***: Candidate customer segments were generated using an unsupervised method, i.e. with no target feature in mind. The *k*-means algorithm was used to detect segments of customers similar to each other ("nearby") in the numerical segment feature space. The elbow method was used to select a good number of segments, and the resulting segments used to create customer segments.
7. ***Segmentation Evaluation and Selection***: The two segmentation methods were evaluated, both individually, and with respect to each other, with several statistical metrics used to approximate each method's ability to generate useful segment. The results of this evaluation process were used to select the better segmentation method, which was determined to be *k*-means.
8. ***Segment Profiling***: The segments generated by *k*-means were used to generating segment profiles. Specifically, the properties of these segments were investigated, specifically with respect to high and low retention customers, and corresponding characteristics identified and discussed.

### Preprocessing



<!-- The following preprocessing steps are worthy of note: -->

<!-- 1. Missing values of the internal company features `DataLastMonth`, `DataOverTenure`, `EquipmentLastMonth`, `EquipmentOverTenure`, `VoiceOverTenure` were set to 0. Missing values of other variables were imputed with the standard strategies of using the mode for categorical features and the mean for numerical features. -->
<!-- 2. Features `Internet` and `HomeOwner` were recoded as ordinal integer features with values 1,...,5 and 0, 1, respectively. The integer ordinal was chosen for `Internet` to correct for inconsistencies in labelling, and intended to reflect tiers of internet services offered by the company. -->

<!-- ### Feature Grouping -->

<!-- As a useful conceptual strategy, to aid the exploration and selection, given the large number of features, the following grouping of features into categories was used: -->

<!-- ```{r group_features, echo=TRUE} -->

<!-- # demographic features not related to personal finances -->
<!-- demo_feats <- sort(c('Gender', 'JobCategory', 'MaritalStatus', 'PoliticalPartyMem',  -->
<!--                     'Retired', 'UnionMember', 'Age', 'EducationYears', 'EmploymentLength', -->
<!--                     'HouseholdSize', 'TownSize', 'CommuteTime')) -->

<!-- # features having to do with personal finances  -->
<!-- fin_feats <- sort(c('LoanDefault', 'HHIncome', 'CardItemsMonthly', 'CardTenure',  -->
<!--                    'CardSpendMonth', 'CreditDebt', 'DebtToIncomeRatio', 'OtherDebt')) -->

<!-- # features related to customer ownership of goods -->
<!-- own_feats <- sort(c('CarBrand', 'CarOwnership', 'OwnsFax', 'OwnsGameSystem',  -->
<!--                    'OwnsMobileDevice', 'OwnsPC', 'CarsOwned', 'CarValue', 'HomeOwner', -->
<!--                    'Internet', 'NumberBirds', 'NumberCats', 'NumberDogs', 'NumberPets')) -->

<!-- # features related to customer behavior, both with respect to company business or otherwise -->
<!-- behave_feats <- sort(c('NewsSubscriber', 'Votes', 'TVWatchingHours', 'ActiveLifestyle')) -->

<!-- # account features not related to billing  -->
<!-- acct_feats <- sort(c('CustomerID', 'CallerID', 'CallForward', 'CallWait', 'CallingCard', -->
<!--                     'EquipmentRental', 'Multiline',  'Pager', 'ThreeWayCalling', 'VM', -->
<!--                     'WirelessData', 'PhoneCoTenure', 'Region', 'DataLastMonth', -->
<!--                     'DataOverTenure', 'EquipmentLastMonth', 'EquipmentOverTenure', -->
<!--                     'VoiceLastMonth', 'VoiceOverTenure')) -->

<!-- # features related to billing -->
<!-- bill_feats <- sort(c('CreditCard', 'EBilling')) -->

<!-- # collect grouped features -->
<!-- feats_by_meaning <- c('CustomerID', demo_feats, fin_feats, own_feats, behave_feats, acct_feats, bill_feats) -->

<!-- # print feature groupings -->
<!-- cat("Demographic: \n\n \t", demo_feats, "\n\n") -->
<!-- cat("Personal Finance: \n\n \t ", fin_feats, "\n\n") -->
<!-- cat("Ownership: \n\n \t", own_feats, "\n\n") -->
<!-- cat("Behavioral: \n\n \t", behave_feats, "\n\n") -->
<!-- cat("Account: \n\n \t", acct_feats, "\n\n") -->
<!-- cat("Billing: \n\n \t", bill_feats, "\n\n") -->
<!-- ``` -->

### Feature Engineering

New derivative features were then created, and preexisting feature transformed. Of particular interest was the preexisting feature `PhoneCoTenure`, which indicated the number of months a customer has been with the company. We used this feature to identify long-term vs short-term, i.e. high-vs-low retention customers by creating a new feature `HighRetention`, indicating which customers had a tenure greater than the 75% percentile, namely 59 months.

See the appendix for distribution plots of `PhoneCoTenure` and `HighRetention`.

```{r tenure_dist_plot}
p <- data %>%
  ggplot(aes(x = PhoneCoTenure)) +
  geom_histogram(binwidth = 1, fill = "darkblue", color = "black") +
  labs(title = "Histogram of PhoneCoTenure",
       x = "PhoneCoTenure",
       y = "Frequency") +
  theme_minimal()

# converts static ggplot plot to dynamic plotly plot - comment when knitting to .docx
ggplotly(p)

# uncomment when knitting to .docx
# p
```

```{r tenure_summary_stats}
skim(data %>% select(PhoneCoTenure))
```

***SIDEKICK: TO DO - `PhoneCoTenure` distribution goes here***

```{r add_hiretent_and_save}
# proportion of customers with maximum tenure
data <- data %>% 
  mutate(HighRetention = as.factor(PhoneCoTenure > 59))


```

```{r hiretent_dist}

retention_prop <- data %>%
  group_by(HighRetention) %>%
  summarise(count = n()) %>%
  mutate(proportion = count / sum(count))


p <- retention_prop %>%
  ggplot(aes(x = HighRetention, y = proportion)) +
  geom_bar(stat = "identity", fill = "darkblue") +
  labs(title = "Distribution of HighRetention",
       x = "HighRetention",
       y = "Proportion") +
  theme_minimal()

# converts static ggplot plot to dynamic plotly plot - comment when knitting to .docx
ggplotly(p)

# uncomment when knitting to .docx
# p
```

The following five new derivative features were also created:

- `TotalDebt = CreditDebt + OtherDebt`: Total customer debt.
- `AvgCardSpendMonth = CardSpendMonth/CardItemsMonthly`: Average monthly credit card spending per item. Set to 0 if `CardItemsMonthly == 0`.
- `AvgValuePerCar = CarValue/CarsOwned`: Average value per car owned. Set to 0 if `CarsOwned == 0`.
- `TechOwnership =  OwnsFax + OwnsGameSystem + OwnsMobileDevice + OwnsPC`: Number of technological items owned out of 4 possible (here the binary ownership features are `0/1` encoded)
- `NumAddOns = Multiline + Pager + ThreeWayCalling + VM`: Number of account add-ons out of 4 possible (again the binary add-on features are `0/1` encoded)

In addition, due to highly skewed distributions, the four features `CardSpendMonth`, `DataOverTenure`, `VoiceOverTenure`, `HHIncome`, `CarValue`. were transformed to standardized versions, i.e. they were transformed by subtracting the mean and dividing by the standard deviation). Although these features were not explicitly used during the segmentation process, they were effectively used, as all segmentation features were numeric and standardized.

It made sense to treat missing values as zero. In particular, we choose to do this for `DataLastMonth`, `DataOverTenure`, `EquipmentLastMonth`, `EquipmentOverTenure`, `VoiceOverTenure`, since presumably the company has access to this information, and if it isn't present, we assumed it can be treated as 0.

Finally for the remaining features, we use a standard imputation strategy, using the mode for categorical features and mean values for numerical features.

### EDA and Feature Selection

The preexisting and engineered features were used for exploratory data analysis, in which a subset of useful customer features was identified for use in segmentation and profiling.

Overall, the ability to provide useful and potentially novel insights through segmentation and profiling was the main criteria in feature selection. In particular, it was surmised that stakeholders and decision makers may be interested in identifying customer that may end up having a long tenure but that currently do not. 

For that reason, certain variables with potentially useful information about internal customer behavior over a long tenure were omitted, namely `DataOverTenure`, `EquipmentOverTenure`, and `VoiceOverTenure`, that is, feature whose future values over a long tenure would be currently unknown. 

Moreover, other time-dependent features such as `Age`, `Employment` were identified as potentially confounding, that is, features with strong associations to long tenure (and thus to the derivative retention feature), and were also omitted.

Finally, the specific interest in using *k*-means segmenting as an unsupervised method, due to its ability to detect unknown patterns, was the next most important criteria, and had a large impact on the choice of features. Primarily, it resulted in a choice of purely numeric features for the segmentation process. Then, using the segments thus constructed, categorical feature characteristics for high and low retention customers were identified and discussed.s

With these criteria in mind, the following hand-picked selection of fifteen customer demographic, behavioral and financial features was used in out our segmentation



```{r seg_target_and_feats, include=TRUE}
# customized set segmentation targets and features
seg_target <- "HighRetention"
seg_feats <- c("CommuteTime", "HouseholdSize",  "TownSize", "CardItemsMonthly",
               "DebtToIncomeRatio", "HHIncome", "CarsOwned", 
               "TVWatchingHours", "Region", "TotalDebt",
               "CardSpendMonth", "HHIncome", "CarValue", "TechOwnership", "NumAddOns")
```

```{r seg_data}

# subset features and target
seg_data <- data %>%
  select(all_of(c(seg_target, seg_feats)))

glimpse(seg_data)
```

### Segmentation Methods

#### Supervised Decision Tree Segmentation

The aim of supervised learning is in general to find meaningful associations between the features and the target. In this case, we employed a supervised-learning-based segmentation method in the hopes of finding a detectable useful pattern between the custom customer features selected for segmentation and the target customer feature `HighRetention`, thus capturing a meaningful association between segments and high and low value customer segments.

##### Decision Tree Diagram

A decision tree algorithm was used to discover statistically meaningful decision rules among the numerical segmentation features. The rules are produced by the algorithm by optimizing a (mathematically defined) criterion which essentially measures how well the rules classify the observations according to the target feature, in this case, the binary customer retention feature `HighRetention`.

<!-- The algorithm is so-named because the resulting decision rules can be seen as a partition of the feature space into segments, or alternatively, as a sequence of choices about how to place observations (in this case customers) into segments, and these rules can be easily visualized in a tree diagram. -->

Several decision trees were fit, and the optimal decision tree was chosen which balanced model complexity and accuracy. The inclusion of model complexity in this choice helps improve the expected ability of the fit to generalize to unseen data, that is, to future customers.

The decision rules obtained by the optimal decision tree were used to number all leaves in the tree diagram in order from left to right (note all nodes contain at least one observation). These are the segment labels, and we can assign observations to these segments based on the decision rules.


```{r dec_tree}
# create formula string
formula_string <- paste(seg_target, "~", paste(seg_feats, collapse = " + "))

# convert to formula object
formula <- as.formula(formula_string)


# Fit the decision tree model using rpart
tree_model <- rpart(formula, data = seg_data, control = rpart.control(cp = 0.0001))

# Find the best cp value based on minimum rel error
best_cp <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]),"CP"]

# Prune the tree using the best cp value
pruned_tree_model <- prune(tree_model, cp=best_cp)

# Visualize the pruned decision tree
rpart.plot(pruned_tree_model, faclen = 0, type=0, extra= 104)
```

***SIDEKICK: TO DO - Decision Tree Diagram goes here***

Note that the decision rules corresponding to this decision tree only involve a small subset of the segmentation features. For this reason, the resulting decision tree segmentation was seen as perhaps less than ideal, given that potentially useful information contained in the other features wasn't utilized.

<!-- The aforementioned complexity minimization criteria employed, which potentially helps reduce generalization error (and thus, reflects a true relationship between features and response, rather than a spurious artifact of the given dataset), often results in such a "pruned tree". -->


<!-- ##### Feature Importances -->

<!-- We include here a plot of segmentation features ranked by importance with respect to their use by the decisions trees from which the optimal tree was selected (we omit a more technical discussion of how these features are selected (that is, what precisely "important" means), and invite the curious reader to research the topic further).  -->

```{r feat_imp}
# Calculate and plot feature importances
vip(pruned_tree_model, method = "model", geom = "point", num_features=15)
```


Note that the most important features are those seen in the decision rules for the pruned tree. 

```{r}
# assign segments based on decision tree leaves
seg_data <- seg_data %>%
  mutate(TreeSeg = case_when(
    NumAddOns < 1 ~ 1, 
    NumAddOns >= 1 & TechOwnership < 2 ~ 2,
    NumAddOns >= 1 & TechOwnership >= 2 & HouseholdSize < 2.1 & HHIncome < 98e+3 ~ 3,
    NumAddOns >= 1 & TechOwnership >= 2 & HouseholdSize < 2.1 & HHIncome >= 98e+3 ~ 4,
    NumAddOns >= 1 & TechOwnership >=2 & HouseholdSize >= 2.1 & HHIncome < 80e+3 & HHIncome >= 18e+3 ~ 5,
    NumAddOns >= 1 & TechOwnership >=2 & HouseholdSize >= 2.1 & HHIncome < 80e+3 & HHIncome < 18e+3 & CommuteTime < 22 ~ 6,
    NumAddOns >= 1 & TechOwnership >=2 & HouseholdSize >= 2.1 & HHIncome < 80e+3 & HHIncome < 18e+3 & CommuteTime >= 22 ~ 7,
    NumAddOns >= 1 & TechOwnership >=2 & HouseholdSize >= 2.1 & HHIncome >= 80e+3 ~ 8
  )) %>%
  mutate(TreeSeg = as.factor(TreeSeg)) %>% # cast to fct - segment is a categorical variable, not ordinal
  relocate(TreeSeg)
```



#### Unsupervised *k*-Means Segmentation

<!-- In general, unsupervised learning methods are employed to capture novel or unexpected relationships amongst features and observations, that is, without the assumptions implicit in the selection of a special "target" to associate the remaining features to. The goal that at least some of these segments should provide some insight about high and low retention customers, that is, that some segments should prove to contain more high or low retention customers than others. -->

To keep the *k*-means segmentation truly unsupervised, we wished to suppress any information related to customer retention in the learning algorithm. Accordingly, the tenure-related features `PhoneCoTenure` and `HighRetention` were omitted. The hope was that in doing so, the resulting segmentation would contain meaningful information about high and low customer retention *thus adding weight* to the segmentation pattern thus discovered (since it contained no assumptions or information about retention, and yet such associations were discovered independently).

<!-- We mentioned previously that *k*-means segmenting can only use numerically encoded features. This is because it relies on a notion of distance between points in the feature space, which does not apply to non-numeric features. -->

##### Scale Data

Given that the features are measured on vastly different scales, it is possible that large scale features can have undue influence on the resulting *k*-means segmenting. Following standard procedures, the segmentation features were thus standardized to reduce this risk.

```{r scale_seg_data}
# encode target as integer
seg_data <- seg_data %>% 
  mutate(HighRetention = case_when(
    HighRetention == FALSE ~ 0,
    HighRetention == TRUE ~ 1
  )) 


# standardize
seg_data_sc <- as_tibble(seg_data %>%
                         select(!c(HighRetention, TreeSeg)) %>% 
                          scale()
                         )

seg_data_sc <- seg_data_sc 

seg_data_sc
```

##### Elbow Method for Selecting Number of Segments

<!-- Naively when using *k*-means to perform segmentation, the practitioner must choose the number *k* of segments beforehand, but it is usually better if this choice can be informed. The elbow method exists for this purpose. A plot of *k* versus a measure of "homogeneity" of the segmented observations is generated -- specifically, this is "total within-segment variation". -->

<!-- The elbow method looks for a elbow (kink) in the *k* vs total within segment variation graph, and selects the minimum just before the kink. Similar to decision tree pruning, this choice of *k* is thought to provide a nice balance between model complexity, in this case, as measured by number of segments,and accuracy, in this case, as measured by how well the segmented observations "group together", i.e. how low the variation is within the segment.  -->


The elbow method for selecting k was employed, and the valueselected. This still balances complexity with the need to detect differences between segments, and provides more fine-grained information when considering high-vs-low retention customers than a smaller number of segments. See the appendix for the elbow plot.

A good deal of trial-and-error justified this choice, revealing it provided a good separation of high vs.low retention customers by segment, as well as a good separation of features, as determined by the variance (spread) across segments of the segment means for each feature (more below).


```{r kmeans_segs}
set.seed(27)
NUM_CLUSTERS <- 8
kclust <- kmeans(seg_data_sc, centers = NUM_CLUSTERS)

# add back in unscaled columns
seg_data_sc$HighRetention <- seg_data$HighRetention
seg_data_sc$TreeSeg <- seg_data$TreeSeg

# add segments to scaled and unscaled numeric features
seg_data_sc$kSeg <- as.factor(kclust$cluster)
seg_data$kSeg <- as.factor(kclust$cluster)

# relocate
seg_data_sc <- seg_data_sc %>% 
  relocate(HighRetention, TreeSeg, kSeg)
seg_data <- seg_data %>% 
  relocate(HighRetention, TreeSeg, kSeg)
```


##### Plots and Analysis

***SIDEKICK: TO DO - Add Distribution Plot***

***SIDEKICK: TO DO - Add Stacked Bar Plot***


## Findings

### Evaluate Segmentation Solutions

In order to evaluate and compare the segmentation solution, we relied on the following evaluation criteria:

1. ***Segment Feature Space Utilization***: How well does the segmentation method make use of the available features?
2. ***Segment Separation***: How well does the segmentation methodseparate different clusters from each other?
3. ***High and Low Retention Discrimination***: How well does the segmentation method allow us to differentiate high and low retention customers by segment, that is, how well does it place and high retention customers into some segments, and low retention customer into others?

#### Feature Space Utilization

The decision tree model only used 5/15 $\approx$ 33% of the total number of features, whereas *k*-means intrinsically makes use of all features.

***Conclusion***: Due to the omission by the decision tree model of 10/15 $\approx$ 67% of the total number of features, *k*-means has clearly better segment feature space utilization.

#### Segment Separation

To measure the degree to which the segments are well-separated from each other, we use two statistical measures of separation, the total and average variance (across segments) of the segment means, which we call "total separation" and "average separation". Specifically, this is the sum and the average of the variances across all features of the featire means across all 8 segments.

The reasoning for the use of this measure is as follows. Since the mean of each feature is a measure of it's "center" (and indeed, the centroid of each segment is the vector of feature means), the variance of means capture how far the within-segment means of each feature are from their overall center. 

The sum and average variance of the feature means taken over all features captures how far the within-segment feature centers are from their overall center, and thus ostensible, from each other.


```{r treeseg_vars}
treeseg_vars <- read_csv('treeseg_sc_summary_stats.csv') %>%
 select(contains('_mean')) %>%
  select(!HighRetention_sc_mean) %>%
 summarise(across(everything(), var, na.rm = TRUE)) %>%
  rename_with(~ paste0("seg_var_", .))
```

```{r kseg_vars}
kseg_vars <- read_csv('kseg_sc_summary_stats.csv') %>%
 select(contains('_mean')) %>%
  select(!HighRetention_sc_mean) %>%
 summarise(across(everything(), var, na.rm = TRUE)) %>%
  rename_with(~ paste0("seg_var_", .))
```

```{r sum_treeseg_vars, include=}
cat("Total Segment Separation for Decision Tree:", sum(treeseg_vars))
```

```{r mean_treeseg_vars}
cat("Average Segment Separation of Decision Tree:", mean(t(treeseg_vars)))
```

```{r sum_kseg_vars}
cat("Total Segment Separation of k-Means:", sum(kseg_vars))
```

```{r mean_kseg_vars}
cat("Average Segment Separation of k-Means:", mean(t(kseg_vars)))
```

***Conclusion***: *k*-means is clearly better at separating segments, with higher total and average segment separation.

#### High and Low Retention Discrimination

To determine how well the segmentation methods separate high and low retention customers, we look at the overall picture provided by proportion of high segmentation customers per segment, per method. When considering these results, note that there is no natural correspondence between the numbers assigned to each segment by each method.

```{r get_treeseg_summary_stats}
# Summarize statistics for each segment
treeseg_summary_stats <- seg_data %>%
  select(!kSeg) %>%
  add_count(TreeSeg) %>%
  group_by(TreeSeg, n) %>%
  summarize(
    across(.cols = everything(), list(mean = ~ mean(.), sd = ~ sd(.)), .names = "{col}_{fn}"),
    HighRetention_Percent = mean(HighRetention) * 100,
    .groups = 'drop'
  ) %>%
  relocate(TreeSeg
           , HighRetention_mean) # put HighRetention up front

```

```{r get_kseg_summary_stats}
# Summarize statistics for each segment
kseg_summary_stats <- seg_data %>%
  select(!TreeSeg) %>%
  add_count(kSeg) %>%
  group_by(kSeg, n) %>%
  summarize(
    across(.cols = everything(), list(mean = ~ mean(.), sd = ~ sd(.)), .names = "{col}_{fn}"),
    HighRetention_Percent = mean(HighRetention) * 100,
    .groups = 'drop'
  ) %>%
  select(-contains("HighRetention_sd")) %>% # remove the unnecessary sd column for HighRetention
  relocate(kSeg, HighRetention_mean) # put HighRetention up front

```

```{r}
retent_by_seg_tb <- tibble(TreeSeg=treeseg_summary_stats$TreeSeg,
                 TreeSegHighRetention_mean=treeseg_summary_stats$HighRetention_mean,
                 kSeg=kseg_summary_stats$kSeg,
                 kSegHighRetention_mean=kseg_summary_stats$HighRetention_mean)
retent_by_seg_tb %>% select(kSeg, kSegHighRetention_mean) %>%
  arrange(desc(kSegHighRetention_mean))
```

We notice that *k*-means appears better able to separate high retention customers, with segments 6 and 8 having roughly 46% and 49%, respectively. The decision tree also has two segments with high percentages of high retention customers, namely 2 and 4, but these are lower, at roughly 40% and 44% respectively. 

The decision tree appears better at separating low retention customers than *k*-means. For *k*-means, only one segment has a low percentage of high retention customers (hence a high percentage of low retention customers), namely segment 4 at roughly 8%, while for the decision tree there are 4 segments with low percentages of high retention customers, between roughly 8-11%.

***Conclusion***: These results are somewhat mixed, however, given the potentially higher value in identifying high retention customers, we give the advantage to *k*-means.

#### Comparison and Segmentation Method Selection

After careful investigation, it was determined to use the k-means segmentation for the following reasons:

1. Better feature space utilization, at 100% vs. 33% of available features.
2. Better segment separation, as measured by higher sum and average variances of segment means of all features collectively.
3. Better high and low retention separation of customers into individual segments.

```{r}
# add data back to main data frame and store
data$kSeg <- seg_data$kSeg
write_csv(data, "New_Customer_Dataset.csv")
```

### Segmentation with Preferred Solution

Having selected  $k$-means segmentation, the resulting eight segments were visualized and investigated, and the results used to build the corresponding customer profiles. 

In this report, some general observations are made about the eight segments and their corresponding profiles, and provide visualizations. The discussion then focuses on the two high retention segments and one low retention segment.

### Overview of Segmentation Results

***SIDEKICK: TO DO - Overview of Results goes here***

See the appendix for summary statistics on the customer profiles, namely, the median values of the numerical segmentation features and the mode of categorical features.

***SIDEKICK: TO DO - Additional Plots Go Here***

### High and Low Retention Segments

As mentioned. Segments 6 and 8 had much higher retention than other segments, about 47% and 49%, respectively. Segment 4 had much much lower retention than other segments, at approximately 9% ***


## Conclusions

<!-- ## Summary Statistics of Decision Tree and k-Means Segments -->


```{r print_treeseg_summary_stats}
treeseg_summary_stats
```

```{r kseg_summary_stats}
kseg_summary_stats
```



***SIDEKICK: TO DO - Remove This From Final Report!***

```{r get_kseg_summary_stats_med_mode}
# Summarize statistics for each segment
kseg_summary_stats <- seg_data %>%
  select(!TreeSeg) %>%
  mutate(kSeg = as.integer(kSeg)) %>%
  group_by(kSeg) %>%
  summarize(
    across(.cols = everything(), list(med = ~ median(.)), .names = "{col}_{fn}"),
    HighRetention_Percent = mean(HighRetention) * 100,
    .groups = 'drop'
  ) %>%
  select(-contains("HighRetention_med")) %>% # remove the unnecessary median for HighRetention
  relocate(kSeg, HighRetention_Percent)
```

```{r kseg_stats_1_5}
kseg_summary_stats[,1:5]
```

```{r kseg_stats_6_9}
kseg_summary_stats[,c(1,6:9)]
```

```{r kseg_stats_10_13}
kseg_summary_stats[,c(1,10:13)]
```

```{r kseg_stats_14_16}
kseg_summary_stats[,c(1,14:16)]
```

## Appendix: Distribution Plots of `PhoneCoTenure` and `HighRetention`

***SIDEKICK: TO DO - Distribution Plots Go Here***

## Appendix: Segment Distribution Plots for Segmentation Methods

```{r by_retention_tree_seg_dist}
# Calculate joint probabilities
by_retention_tree_seg_dist <- seg_data %>%
  group_by(TreeSeg, HighRetention) %>%
  summarize(Count = n(), .groups = 'drop') %>%
  mutate(JointProbability = Count / sum(Count))


by_retention_tree_seg_dist
```


```{r by_retention_kseg_dist}
# Calculate joint probabilities
by_retention_kseg_dist <- seg_data %>%
  group_by(kSeg, HighRetention) %>%
  summarize(Count = n(), .groups = 'drop') %>%
  mutate(JointProbability = Count / sum(Count))

# Print the joint probabilities of high retention costy
by_retention_kseg_dist
```

## Appendix: High and Low Retention Segment Summary Stats

### Numerical Features

```{r get_hilo_retent_seg_feat_stats}
hilo_retent_summary_stats <- kseg_summary_stats %>%
  filter(kSeg %in% c(4, 6, 8))
```

```{r hilo_retent_seg_feat_stats_1_5}
hilo_retent_summary_stats[,1:5]
```

```{r hilo_retent_seg_feat_stats_6_9}
hilo_retent_summary_stats[,c(1,6:9)]
```

```{r hilo_retent_seg_feat_stats_10_13}
hilo_retent_summary_stats[,c(1,10:13)]
```

```{r hilo_retent_seg_feat_stats_14_16}
hilo_retent_summary_stats[,c(1,14:16)]
```

### Categorical Features

```{r get_hilo_retent_fct_feat_stats}
# add segments to original dataframe
data$kSeg <- seg_data$kSeg

# Define a robust mode function that always returns a character
mode_function <- function(x) {
  uniq_x <- unique(x)
  mode_val <- uniq_x[which.max(tabulate(match(x, uniq_x)))]
  return(as.character(mode_val))
}

hilo_fct_summary_stats <- data %>%
  filter(kSeg %in% c(4, 6, 8)) %>%
  group_by(kSeg) %>%
  summarize(
    across(.cols = where(is.factor), .fns = list(mode = ~ mode_function(.)), .names = "{col}_{fn}"),
    .groups = 'drop'
  ) %>%
  relocate(kSeg)
```

```{r hilo_retent_fct_feat_stats_1_5, include=TRUE, echo=FALSE}
hilo_fct_summary_stats[,1:5]
```

```{r hilo_retent_fct_feat_stats_6_9, include=TRUE, echo=FALSE}
hilo_fct_summary_stats[,c(1,6:9)]
```

```{r hilo_retent_fct_feat_stats_9_12, include=TRUE, echo=FALSE}
hilo_fct_summary_stats[,c(1,9:12)]
```

```{r hilo_retent_fct_feat_stats_13_16, include=TRUE, echo=FALSE}
hilo_fct_summary_stats[,c(1,13:16)]
```

```{r hilo_retent_fct_feat_stats_17_20, include=TRUE, echo=FALSE}
hilo_fct_summary_stats[,c(1,17:20)]
```

```{r hilo_retent_fct_feat_stats_21_24, include=TRUE, echo=FALSE}
hilo_fct_summary_stats[,c(1,21:24)]
```

```{r hilo_retent_fct_feat_stats_24_27, include=TRUE, echo=FALSE}
hilo_fct_summary_stats[,c(1,24:27)]
```

```{r hilo_retent_fct_feat_stats_28_31, include=TRUE, echo=FALSE}
hilo_fct_summary_stats[,c(1,28:31)]
```
